---
title: "ST558-2024-Project-Final-modeling"
author: "Siyuan Su"
format: html
editor: visual
---

## Modeling the ethiology of diabetes

As we saw it in the EDA analysis, there are several patho-physiological (High blood pressure, high cholestoral, difficult to walk, aging), social-economic (Education and Income) and life-style factors (achohol consumption) associated with occurrence of diabetes. It is not surprising, given diabetes arise from the inability of body to deal with the metabolism of sugar, which becomes readily available in modern daily life, in contrast to the environment where *homo sapiens* were evolutionized and refined, where food was in scarcity. However, there is no single factor that could explain more than 30% of diabetes incidence. Here we would like to create a model to further characterize the relationship between the incidence of diabetes and its predictors quantitatively.

We start out by dividing our data set into training and testing set in a 70/30 divide.

```{r}
library (tidymodels)
diabetesdata_split <- initial_split (diabetesdata, prop = 0.7)
```

Now we could create a 5-fold CV on training data set.

```{r}
diabetesdata_train <- training (diabetesdata_split)
diabetesdata_test <- testing (diabetesdata_split)
diabetesdata_5_fold <- vfold_cv(diabetesdata_train, 5)
```

## Fitting the model using classificaton tree model.

Tree models provide very intuitive and easy-to-interpret modeling and incorporates the contribution of different predictors. Tree-based model split up the space in the predictor into different segments, each neighbouring predictor space could have wildly different outcome.

Specifically with this project, the goal of the decision tree is to predict the subjects' diabetic status (whether they are diabetic or not) through the values of predictors. Compared with the linear regression model, which provides output through sometimes complicated formula, the decision tree used a binary decision style that directs the user to make prediction in a step-by-step fashion. Depending on the property of the outcome,the decision tree model comprises both regression tree (when outcome is numeric) and classification tree (when outcome is categorical). Here because the outcome: **Diabetes_binary** is a categorical variable, we have to fit this to a classification tree model.

We could start building the recipe for the classification tree model using the *parsnip* package.

```{r}
diabetes_clstr_rec <- recipe (Diabetes_binary ~ ., data = diabetesdata_train) |>
  step_dummy(all_factor_predictors())
```

Forgot to mention another advantage of the tree model is no need to include interaction terms, because the tree account for the interaction itself. Now we could set the model and engine. There are three tuning parameters for us to tune: 1) tree_depth: Tree Depth (type: integer, default: 30L) 2) min_n: Minimal Node Size (type: integer, default: 2L) 3) cost_complexity: Cost-Complexity Parameter (type: double, default: 0.01) If we want to use CV to choose one of these, we can set its value to tune() when creating the model. Let’s use tree_depth and cost_complexity as our tuning parameters and set our min_n to 50.

In the case of decision_tree() we also need to tell tidymodels whether we are doing a regression task vs a classification task. This is done via set_mode().

```{r}
diabetes_clstr_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
diabetes_clstr_mod
```

Having set up the model and the receipe we could now set up our work flow

```{r}
diabetes_clstr_wkf <- workflow() |>
  add_recipe(diabetes_clstr_rec) |>
  add_model(diabetes_clstr_mod)
```

Now we can use **tune_grid()** on our **diabetesdata_5_fold** object. We just need to create a tuning grid to fit our models with. To reduce our computation burden, I will set the search for cost complexity and tree depth to a 4x4 level grid.

```{r}
diabetes_clstr_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(4, 4))

diabetes_clstr_fits <- diabetes_clstr_wkf |> 
  tune_grid(resamples = diabetesdata_5_fold,
            grid = diabetes_clstr_grid)

diabetes_clstr_fits
 diabetes_clstr_fits |> collect_metrics ()
```

Here we did not see the metric we wanted to find **mn_log_loss**. Then we could name the metrics we wanted to see and try rerun the tune_grid process using multiple core and a progress indicator.

```{r}
metrics <- metric_set(accuracy, mn_log_loss)

library(doParallel)
registerDoParallel(cores = 6)

diabetes_clstr_fits <- diabetes_clstr_wkf |> 
  tune_grid(resamples = diabetesdata_5_fold,
            grid = diabetes_clstr_grid,
            metrics = metrics,
            control = control_grid(verbose = TRUE))

stopImplicitCluster()

diabetes_clstr_fits |> collect_metrics ()

diabetes_clstr_fit_results <- diabetes_clstr_fits |>collect_metrics() |>
  filter (.metric == "mn_log_loss") |> arrange (mean)
diabetes_clstr_fit_results
```

From the grid-based computation, we computed that the best combination of cost complexity and tree depth is c (0.0000000001, 10). It looks like a minimum cost complexity and a moderately big tree depth.
The function *select_best()* could also be used to select the lowest rmse.

```{r}
diabetes_clstr_bestfit <- select_best (diabetes_clstr_fits, metric = "mn_log_loss")
diabetes_clstr_bestfit
```

Now we have got the best fit of this classification tree model, we could use this model to update our work flow.

```{r}
diabetes_clstr_final_wkf <- diabetes_clstr_wkf |>
  finalize_workflow(diabetes_clstr_bestfit)

registerDoParallel(cores = 6)

diabetes_clstr_final_fit <- diabetes_clstr_final_wkf |>
  last_fit(diabetesdata_split,
           metrics = metrics)
diabetes_clstr_final_fit |> collect_metrics()

stopImplicitCluster()
```

Now we could see how this tree works using the package **rpart.plot**

```{r}
library (rpart.plot)
diabetes_clstr_final_model <- extract_workflow(diabetes_clstr_final_fit) 
diabetes_clstr_final_model
diabetes_clstr_final_model |>
  extract_fit_engine() |>
  rpart.plot::rpart.plot(roundint = FALSE)
```

This is a user-friendly classification tree model, albeit it has For example, if you have high blood pressure, BMI higher than 32, high blood cholestrol level, have difficulty to walk and bad general health, experienced heart disease or heart attack and had even experienced stroke, you will have very high chance of getting diagnosed with diabetes.

To sum up, there are several advantages of using tree as the prediction model: 1) Tree models are **simple to understand** and easy to interpret output. Even with the tree model of 10 layers, users could follow simple guidance to predict individual's diabetic status. 2) Predictors **don’t need to be scaled**. That makes the results even more predictable. 3) No need to consider the **interaction** between variables. 4) No need to select variables, tree models could select *useful* variable by themselves.

At the same time we need to bear in mind there are several drawbacks in the simple tree model as well. 1) Small changes in data can vastly change tree. 2) Simple tree model used a greedy algorithm that only looks at one step forward. With this setting it is possible the local best solution could be different from the global best solution. 3) Need to prune or use CV to determine the model. Here we used cross validation to figure out the optimal size of the tree, which could take quite some time for big data sets.

To overcome the drawbacks, we used emsembled methods to reduce the variation of the tree models. One way to do it is through bagged tree model. Bagged tree model is basically to build many trees based on bootstrapped samples, and then average the trees to get a final tree. As we learned in the statistics course the variation of stats from averaged samples are often much smaller than the variations of stats from samples themselves, it is possibly truth as in the result of the tree model as well. The idea of random forest tree models were initially based on the bagged tree models.

## Fitting the model using the random forest method.

Random forest model also used averaged trees from bootstrapped samples. In addition, to avoid the impact of single or a small number of "strong" predictors, random forest randomly picked up predictors when building trees. Because the presence of one or a small number of strong predictors will direct trees towards one specific way, it will significantly increase variance. It seems random forest have somewhat alleviate this problem. Whether random forest works in our specific data set still needs to be compared with single tree method. Now let's work on the random forest model.

I have heard about that random forest model takes generally a long time to run, so I decided to adpot several strategies to avoid that. 1) Randomly select 20% of my data set from the index of rows and tune the number of predictors to run based on this data set. 2) Setting up a monitor to know the progress of computing.

First we will randomly select 20% of my data set.

```{r}
set.seed(123)  # For reproducibility
diabetesdata_sample <- diabetesdata |> sample_frac(0.2)
diabetesdata_5_fold_sample <- vfold_cv(diabetesdata_sample, 5)
```

We could keep the previous recipe, and then we tune the grid to get the number of variables. In the *set_engine* function we could also set argument *verbose* to TRUE to allow monitor of the progress.

```{r}
diabetes_rf_rec <- diabetes_clstr_rec 
diabetes_rf_mod <- rand_forest(mtry = tune()) |>
  set_engine("ranger", verbose = TRUE) |>
  set_mode("classification")
```

With the recipe and model we could build our workflow

```{r}
diabetes_rf_wkf <- workflow() |>
  add_recipe (diabetes_rf_rec) |>
  add_model (diabetes_rf_mod)
```

With the new random forest work flow, we could implement this on our sample CV fold data

```{r}
library (progress)
library (ranger)
 diabetes_sample_fit <- diabetes_rf_wkf |>
  tune_grid(resamples = diabetesdata_5_fold_sample, 
            grid = 10,
            control = control_grid(verbose = TRUE),
            metrics = metric_set(accuracy, mn_log_loss))
```

Now that we finished calculating we could sort the result using the **mn log loss**.

```{r}
diabetes_sample_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |> 
  arrange(mean)
```

Then we could get the best-performing data frame
```{r}
diabetes_rf_best_params <- select_best(diabetes_sample_fit, metric = "mn_log_loss")
diabetes_rf_best_params
```

Ultimately we could refit the random forest with our best parameters on the entire data set. To speed up the process, try implement multicore processing to enable parallel computation, although it is of best practise to use parallel computing at grid stage.

```{r}
registerDoParallel(cores = 6)

diabetes_rf_final_wkf <- diabetes_rf_wkf |>
  finalize_workflow(diabetes_rf_best_params)

diabetes_rf_final_fit <- diabetes_rf_final_wkf |>
  last_fit(diabetesdata_split, 
           metrics = metric_set(accuracy, mn_log_loss),
           )

stopImplicitCluster()
```

Now we could compare the performance of the simple tree model and random forest model.

```{r}
diabetes_rf_final_model <- extract_workflow(diabetes_rf_final_fit) 
diabetes_clstr_final_fit |> collect_metrics()
diabetes_rf_final_fit |> collect_metrics()
```

We could see the random forest model improved the log loss value over the simple model, while the interpretability was sacrificed.

