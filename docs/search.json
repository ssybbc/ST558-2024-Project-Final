[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558-2024-Project-Final-modeling",
    "section": "",
    "text": "As we saw it in the EDA analysis, there are several patho-physiological (High blood pressure, high cholestoral, difficult to walk, aging), social-economic (Education and Income) and life-style factors (achohol consumption) associated with occurrence of diabetes. It is not surprising, given diabetes arise from the inability of body to deal with the metabolism of sugar, which becomes readily available in modern daily life, in contrast to the environment where homo sapiens were evolutionized and refined, where food was in scarcity. However, there is no single factor that could explain more than 30% of diabetes incidence. Here we would like to create a model to further characterize the relationship between the incidence of diabetes and its predictors quantitatively.\nWe start out by dividing our data set into training and testing set in a 70/30 divide.\n\nlibrary (tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary (dplyr)\ndiabetesdata &lt;- read.csv (\"data\\\\diabetes_binary_health_indicators_BRFSS2015.csv\")\ndiabetesdata &lt;- diabetesdata |&gt; mutate (across (-c(MentHlth, PhysHlth, BMI), as.factor))\nlibrary (tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\ndiabetesdata_split &lt;- initial_split (diabetesdata, prop = 0.7)\n\nNow we could create a 5-fold CV on training data set.\n\ndiabetesdata_train &lt;- training (diabetesdata_split)\ndiabetesdata_test &lt;- testing (diabetesdata_split)\ndiabetesdata_5_fold &lt;- vfold_cv(diabetesdata_train, 5)"
  },
  {
    "objectID": "Modeling.html#modeling-the-ethiology-of-diabetes",
    "href": "Modeling.html#modeling-the-ethiology-of-diabetes",
    "title": "ST558-2024-Project-Final-modeling",
    "section": "",
    "text": "As we saw it in the EDA analysis, there are several patho-physiological (High blood pressure, high cholestoral, difficult to walk, aging), social-economic (Education and Income) and life-style factors (achohol consumption) associated with occurrence of diabetes. It is not surprising, given diabetes arise from the inability of body to deal with the metabolism of sugar, which becomes readily available in modern daily life, in contrast to the environment where homo sapiens were evolutionized and refined, where food was in scarcity. However, there is no single factor that could explain more than 30% of diabetes incidence. Here we would like to create a model to further characterize the relationship between the incidence of diabetes and its predictors quantitatively.\nWe start out by dividing our data set into training and testing set in a 70/30 divide.\n\nlibrary (tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary (dplyr)\ndiabetesdata &lt;- read.csv (\"data\\\\diabetes_binary_health_indicators_BRFSS2015.csv\")\ndiabetesdata &lt;- diabetesdata |&gt; mutate (across (-c(MentHlth, PhysHlth, BMI), as.factor))\nlibrary (tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\ndiabetesdata_split &lt;- initial_split (diabetesdata, prop = 0.7)\n\nNow we could create a 5-fold CV on training data set.\n\ndiabetesdata_train &lt;- training (diabetesdata_split)\ndiabetesdata_test &lt;- testing (diabetesdata_split)\ndiabetesdata_5_fold &lt;- vfold_cv(diabetesdata_train, 5)"
  },
  {
    "objectID": "Modeling.html#fitting-the-model-using-classificaton-tree-model.",
    "href": "Modeling.html#fitting-the-model-using-classificaton-tree-model.",
    "title": "ST558-2024-Project-Final-modeling",
    "section": "Fitting the model using classificaton tree model.",
    "text": "Fitting the model using classificaton tree model.\nTree models provide very intuitive and easy-to-interpret modeling and incorporates the contribution of different predictors. Tree-based model split up the space in the predictor into different segments, each neighbouring predictor space could have wildly different outcome.\nSpecifically with this project, the goal of the decision tree is to predict the subjects’ diabetic status (whether they are diabetic or not) through the values of predictors. Compared with the linear regression model, which provides output through sometimes complicated formula, the decision tree used a binary decision style that directs the user to make prediction in a step-by-step fashion. Depending on the property of the outcome,the decision tree model comprises both regression tree (when outcome is numeric) and classification tree (when outcome is categorical). Here because the outcome: Diabetes_binary is a categorical variable, we have to fit this to a classification tree model.\nWe could start building the recipe for the classification tree model using the parsnip package.\n\ndiabetes_clstr_rec &lt;- recipe (Diabetes_binary ~ ., data = diabetesdata_train) |&gt;\n  step_dummy(all_factor_predictors())\n\nForgot to mention another advantage of the tree model is no need to include interaction terms, because the tree account for the interaction itself. Now we could set the model and engine. There are three tuning parameters for us to tune: 1) tree_depth: Tree Depth (type: integer, default: 30L) 2) min_n: Minimal Node Size (type: integer, default: 2L) 3) cost_complexity: Cost-Complexity Parameter (type: double, default: 0.01) If we want to use CV to choose one of these, we can set its value to tune() when creating the model. Let’s use tree_depth and cost_complexity as our tuning parameters and set our min_n to 50.\nIn the case of decision_tree() we also need to tell tidymodels whether we are doing a regression task vs a classification task. This is done via set_mode().\n\ndiabetes_clstr_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\ndiabetes_clstr_mod\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\nHaving set up the model and the receipe we could now set up our work flow\n\ndiabetes_clstr_wkf &lt;- workflow() |&gt;\n  add_recipe(diabetes_clstr_rec) |&gt;\n  add_model(diabetes_clstr_mod)\n\nNow we can use tune_grid() on our diabetesdata_5_fold object. We just need to create a tuning grid to fit our models with. To reduce our computation burden, I will set the search for cost complexity and tree depth to a 4x4 level grid. In addition in order to accelerate calculation I enabled parallel calculation by using 6 cores together using the registerDoParallel() command.\n\ndiabetes_clstr_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(4, 4))\n\nmetrics &lt;- metric_set(accuracy, mn_log_loss)\n\nlibrary(doParallel)\n\nWarning: package 'doParallel' was built under R version 4.3.3\n\n\nLoading required package: foreach\n\n\nWarning: package 'foreach' was built under R version 4.3.2\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nWarning: package 'iterators' was built under R version 4.3.2\n\n\nLoading required package: parallel\n\nregisterDoParallel(cores = 6)\n\ndiabetes_clstr_fits &lt;- diabetes_clstr_wkf |&gt; \n  tune_grid(resamples = diabetesdata_5_fold,\n            grid = diabetes_clstr_grid,\n            metrics = metrics,\n            control = control_grid(verbose = TRUE))\n\nstopImplicitCluster()\n\ndiabetes_clstr_fits |&gt; collect_metrics ()\n\n# A tibble: 32 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 accuracy    binary     0.861     5 9.43e-4 Prepro…\n 2    0.0000000001          1 mn_log_loss binary     0.403     5 1.72e-3 Prepro…\n 3    0.0000001             1 accuracy    binary     0.861     5 9.43e-4 Prepro…\n 4    0.0000001             1 mn_log_loss binary     0.403     5 1.72e-3 Prepro…\n 5    0.0001                1 accuracy    binary     0.861     5 9.43e-4 Prepro…\n 6    0.0001                1 mn_log_loss binary     0.403     5 1.72e-3 Prepro…\n 7    0.1                   1 accuracy    binary     0.861     5 9.43e-4 Prepro…\n 8    0.1                   1 mn_log_loss binary     0.403     5 1.72e-3 Prepro…\n 9    0.0000000001          5 accuracy    binary     0.864     5 1.29e-3 Prepro…\n10    0.0000000001          5 mn_log_loss binary     0.357     5 1.59e-3 Prepro…\n# ℹ 22 more rows\n\ndiabetes_clstr_fit_results &lt;- diabetes_clstr_fits |&gt;collect_metrics() |&gt;\n  filter (.metric == \"mn_log_loss\") |&gt; arrange (mean)\ndiabetes_clstr_fit_results\n\n# A tibble: 16 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001         10 mn_log_loss binary     0.335     5 0.00184 Prepro…\n 2    0.0000001            10 mn_log_loss binary     0.335     5 0.00184 Prepro…\n 3    0.0001               10 mn_log_loss binary     0.344     5 0.00243 Prepro…\n 4    0.0001               15 mn_log_loss binary     0.352     5 0.00155 Prepro…\n 5    0.0000000001          5 mn_log_loss binary     0.357     5 0.00159 Prepro…\n 6    0.0000001             5 mn_log_loss binary     0.357     5 0.00159 Prepro…\n 7    0.0001                5 mn_log_loss binary     0.357     5 0.00159 Prepro…\n 8    0.0000000001         15 mn_log_loss binary     0.378     5 0.00280 Prepro…\n 9    0.0000001            15 mn_log_loss binary     0.378     5 0.00280 Prepro…\n10    0.0000000001          1 mn_log_loss binary     0.403     5 0.00172 Prepro…\n11    0.0000001             1 mn_log_loss binary     0.403     5 0.00172 Prepro…\n12    0.0001                1 mn_log_loss binary     0.403     5 0.00172 Prepro…\n13    0.1                   1 mn_log_loss binary     0.403     5 0.00172 Prepro…\n14    0.1                   5 mn_log_loss binary     0.403     5 0.00172 Prepro…\n15    0.1                  10 mn_log_loss binary     0.403     5 0.00172 Prepro…\n16    0.1                  15 mn_log_loss binary     0.403     5 0.00172 Prepro…\n\n\nIn the fitting results, we chose the combinations with least log loss. From the grid-based computation, we computed that the best combination of cost complexity and tree depth is c (0.0000000001, 10). It looks like a minimum cost complexity and a moderately big tree depth.\nThe function select_best() could also be used to select the lowest log loss.\n\ndiabetes_clstr_bestfit &lt;- select_best (diabetes_clstr_fits, metric = \"mn_log_loss\")\ndiabetes_clstr_bestfit\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         10 Preprocessor1_Model09\n\n\nNow we have got the best fit of this classification tree model, we could use this model to update our work flow.\n\ndiabetes_clstr_final_wkf &lt;- diabetes_clstr_wkf |&gt;\n  finalize_workflow(diabetes_clstr_bestfit)\n\nregisterDoParallel(cores = 6)\n\ndiabetes_clstr_final_fit &lt;- diabetes_clstr_final_wkf |&gt;\n  last_fit(diabetesdata_split,\n           metrics = metrics)\n\nWarning: package 'rpart' was built under R version 4.3.3\n\ndiabetes_clstr_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.862 Preprocessor1_Model1\n2 mn_log_loss binary         0.337 Preprocessor1_Model1\n\nstopImplicitCluster()\n\nNow we could see how this tree works using the package rpart.plot\n\nlibrary (rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\ndiabetes_clstr_final_model &lt;- extract_workflow(diabetes_clstr_final_fit) \ndiabetes_clstr_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 177576 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 177576 24699 0 (0.86091026 0.13908974)  \n     2) HighBP_X1&lt; 0.5 101387  6087 0 (0.93996272 0.06003728)  \n       4) DiffWalk_X1&lt; 0.5 91682  4456 0 (0.95139722 0.04860278)  \n         8) GenHlth_X4&lt; 0.5 86886  3710 0 (0.95730037 0.04269963)  \n          16) GenHlth_X3&lt; 0.5 64439  1723 0 (0.97326153 0.02673847)  \n            32) GenHlth_X5&lt; 0.5 63616  1579 0 (0.97517920 0.02482080) *\n            33) GenHlth_X5&gt;=0.5 823   144 0 (0.82503038 0.17496962)  \n              66) BMI&lt; 27.5 494    54 0 (0.89068826 0.10931174) *\n              67) BMI&gt;=27.5 329    90 0 (0.72644377 0.27355623)  \n               134) HeartDiseaseorAttack_X1&lt; 0.5 283    64 0 (0.77385159 0.22614841)  \n                 268) Income_X2&lt; 0.5 246    45 0 (0.81707317 0.18292683) *\n                 269) Income_X2&gt;=0.5 37    18 1 (0.48648649 0.51351351)  \n                   538) BMI&gt;=32.5 24    10 0 (0.58333333 0.41666667)  \n                    1076) BMI&lt; 40.5 17     5 0 (0.70588235 0.29411765) *\n                    1077) BMI&gt;=40.5 7     2 1 (0.28571429 0.71428571) *\n                   539) BMI&lt; 32.5 13     4 1 (0.30769231 0.69230769) *\n               135) HeartDiseaseorAttack_X1&gt;=0.5 46    20 1 (0.43478261 0.56521739)  \n                 270) BMI&lt; 32.5 28    11 0 (0.60714286 0.39285714)  \n                   540) PhysActivity_X1&lt; 0.5 14     3 0 (0.78571429 0.21428571) *\n                   541) PhysActivity_X1&gt;=0.5 14     6 1 (0.42857143 0.57142857) *\n                 271) BMI&gt;=32.5 18     3 1 (0.16666667 0.83333333) *\n          17) GenHlth_X3&gt;=0.5 22447  1987 0 (0.91148038 0.08851962)  \n            34) HighChol_X1&lt; 0.5 14949   989 0 (0.93384173 0.06615827)  \n              68) BMI&lt; 30.5 10715   586 0 (0.94531031 0.05468969) *\n              69) BMI&gt;=30.5 4234   403 0 (0.90481814 0.09518186)  \n               138) Age_X10&lt; 0.5 3994   354 0 (0.91136705 0.08863295)  \n                 276) Age_X9&lt; 0.5 3690   297 0 (0.91951220 0.08048780) *\n                 277) Age_X9&gt;=0.5 304    57 0 (0.81250000 0.18750000)  \n                   554) BMI&lt; 34.5 189    24 0 (0.87301587 0.12698413) *\n                   555) BMI&gt;=34.5 115    33 0 (0.71304348 0.28695652)  \n                    1110) HeartDiseaseorAttack_X1&lt; 0.5 107    28 0 (0.73831776 0.26168224) *\n                    1111) HeartDiseaseorAttack_X1&gt;=0.5 8     3 1 (0.37500000 0.62500000) *\n               139) Age_X10&gt;=0.5 240    49 0 (0.79583333 0.20416667) *\n            35) HighChol_X1&gt;=0.5 7498   998 0 (0.86689784 0.13310216)  \n              70) BMI&lt; 27.5 3682   372 0 (0.89896795 0.10103205)  \n               140) MentHlth&gt;=0.5 1217    72 0 (0.94083813 0.05916187)  \n                 280) BMI&lt; 26.5 966    47 0 (0.95134576 0.04865424) *\n                 281) BMI&gt;=26.5 251    25 0 (0.90039841 0.09960159)  \n                   562) Age_X11&lt; 0.5 229    19 0 (0.91703057 0.08296943) *\n                   563) Age_X11&gt;=0.5 22     6 0 (0.72727273 0.27272727)  \n                    1126) PhysActivity_X1&gt;=0.5 15     2 0 (0.86666667 0.13333333) *\n                    1127) PhysActivity_X1&lt; 0.5 7     3 1 (0.42857143 0.57142857) *\n               141) MentHlth&lt; 0.5 2465   300 0 (0.87829615 0.12170385) *\n              71) BMI&gt;=27.5 3816   626 0 (0.83595388 0.16404612)  \n               142) Age_X4&gt;=0.5 247    12 0 (0.95141700 0.04858300) *\n\n...\nand 572 more lines.\n\ndiabetes_clstr_final_model |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\n\n\nThis is a user-friendly classification tree model, albeit it has For example, if you have high blood pressure, BMI higher than 32, high blood cholestrol level, have difficulty to walk and bad general health, experienced heart disease or heart attack and had even experienced stroke, you will have very high chance of getting diagnosed with diabetes.\nTo sum up, there are several advantages of using tree as the prediction model: 1) Tree models are simple to understand and easy to interpret output. Even with the tree model of 10 layers, users could follow simple guidance to predict individual’s diabetic status. 2) Predictors don’t need to be scaled. That makes the results even more predictable. 3) No need to consider the interaction between variables. 4) No need to select variables, tree models could select useful variable by themselves.\nAt the same time we need to bear in mind there are several drawbacks in the simple tree model as well. 1) Small changes in data can vastly change tree. 2) Simple tree model used a greedy algorithm that only looks at one step forward. With this setting it is possible the local best solution could be different from the global best solution. 3) Need to prune or use CV to determine the model. Here we used cross validation to figure out the optimal size of the tree, which could take quite some time for big data sets.\nTo overcome the drawbacks, we used emsembled methods to reduce the variation of the tree models. One way to do it is through bagged tree model. Bagged tree model is basically to build many trees based on bootstrapped samples, and then average the trees to get a final tree. As we learned in the statistics course the variation of stats from averaged samples are often much smaller than the variations of stats from samples themselves, it is possibly truth as in the result of the tree model as well. The idea of random forest tree models were initially based on the bagged tree models."
  },
  {
    "objectID": "Modeling.html#fitting-the-model-using-the-random-forest-method.",
    "href": "Modeling.html#fitting-the-model-using-the-random-forest-method.",
    "title": "ST558-2024-Project-Final-modeling",
    "section": "Fitting the model using the random forest method.",
    "text": "Fitting the model using the random forest method.\nRandom forest model also used averaged trees from bootstrapped samples. In addition, to avoid the impact of single or a small number of “strong” predictors, random forest randomly picked up predictors when building trees. Because the presence of one or a small number of strong predictors will direct trees towards one specific way, it will significantly increase variance. It seems random forest have somewhat alleviate this problem. Whether random forest works in our specific data set still needs to be compared with single tree method. Now let’s work on the random forest model.\nI have heard about that random forest model takes generally a long time to run, so I decided to adpot several strategies to avoid that. 1) Randomly select 20% of my data set from the index of rows and tune the number of predictors to run based on this data set. 2) Setting up a monitor to know the progress of computing.\nFirst we will randomly select 20% of my data set.\n\nset.seed(123)  # For reproducibility\ndiabetesdata_sample &lt;- diabetesdata |&gt; sample_frac(0.2)\ndiabetesdata_5_fold_sample &lt;- vfold_cv(diabetesdata_sample, 5)\n\nWe could keep the previous recipe, and then we tune the grid to get the number of variables. In the set_engine function we could also set argument verbose to TRUE to allow monitor of the progress.\n\ndiabetes_rf_rec &lt;- diabetes_clstr_rec \ndiabetes_rf_mod &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\", verbose = TRUE) |&gt;\n  set_mode(\"classification\")\n\nWith the recipe and model we could build our workflow\n\ndiabetes_rf_wkf &lt;- workflow() |&gt;\n  add_recipe (diabetes_rf_rec) |&gt;\n  add_model (diabetes_rf_mod)\n\nWith the new random forest work flow, we could implement this on our sample CV fold data. Here we used control = control_grid (verbose =TRUE) option to enable the monitoring of this process.\n\nlibrary (progress)\n\nWarning: package 'progress' was built under R version 4.3.3\n\nlibrary (ranger)\n\nWarning: package 'ranger' was built under R version 4.3.3\n\nregisterDoParallel(cores = 6)\n\n diabetes_sample_fit &lt;- diabetes_rf_wkf |&gt;\n  tune_grid(resamples = diabetesdata_5_fold_sample, \n            grid = 10,\n            control = control_grid(verbose = TRUE),\n            metrics = metric_set(accuracy, mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n stopImplicitCluster()\n\nNow that we finished calculating we could sort the result using the mn log loss.\n\ndiabetes_sample_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt; \n  arrange(mean)\n\n# A tibble: 10 × 7\n    mtry .metric     .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     6 mn_log_loss binary     0.321     5 0.00293 Preprocessor1_Model03\n 2    13 mn_log_loss binary     0.331     5 0.00380 Preprocessor1_Model02\n 3    16 mn_log_loss binary     0.332     5 0.00320 Preprocessor1_Model10\n 4    23 mn_log_loss binary     0.343     5 0.00501 Preprocessor1_Model06\n 5    27 mn_log_loss binary     0.344     5 0.00557 Preprocessor1_Model01\n 6    28 mn_log_loss binary     0.344     5 0.00516 Preprocessor1_Model05\n 7    36 mn_log_loss binary     0.349     5 0.00567 Preprocessor1_Model08\n 8    38 mn_log_loss binary     0.350     5 0.00593 Preprocessor1_Model07\n 9    44 mn_log_loss binary     0.354     5 0.00752 Preprocessor1_Model04\n10     1 mn_log_loss binary     0.356     5 0.00283 Preprocessor1_Model09\n\n\nThen we could get the best-performing data frame\n\ndiabetes_rf_best_params &lt;- select_best(diabetes_sample_fit, metric = \"mn_log_loss\")\ndiabetes_rf_best_params\n\n# A tibble: 1 × 2\n   mtry .config              \n  &lt;int&gt; &lt;chr&gt;                \n1     6 Preprocessor1_Model03\n\n\nUltimately we could refit the random forest with our best parameters on the entire data set. To speed up the process, try implement multicore processing to enable parallel computation, although it is of best practise to use parallel computing at grid stage.\n\nregisterDoParallel(cores = 6)\n\ndiabetes_rf_final_wkf &lt;- diabetes_rf_wkf |&gt;\n  finalize_workflow(diabetes_rf_best_params)\n\ndiabetes_rf_final_fit &lt;- diabetes_rf_final_wkf |&gt;\n  last_fit(diabetesdata_split, \n           metrics = metric_set(accuracy, mn_log_loss),\n           )\n\nstopImplicitCluster()\n\nNow we could compare the performance of the simple tree model and random forest model.\n\ndiabetes_rf_final_model &lt;- extract_workflow(diabetes_rf_final_fit) \ndiabetes_clstr_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.862 Preprocessor1_Model1\n2 mn_log_loss binary         0.337 Preprocessor1_Model1\n\ndiabetes_rf_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.866 Preprocessor1_Model1\n2 mn_log_loss binary         0.320 Preprocessor1_Model1\n\n\nWe could see the random forest model improved the log loss value over the simple model, while the interpretability was sacrificed."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558-2024-Project-Final",
    "section": "",
    "text": "Diabetes is a wide-spread disease, especially in the US. Although genetic plays a role in the ethiology of diabetes, it has been estimated around 90% to 95% of diabetes cases could be attributed to life-style factors, like diet, weight management and physical activity.This data set aimed to providel links between patients’ diabetic conditions, life pattern behavior and social-economic status.\nFor this project, a csv of the data set available on Kaggle for the year 2015 was used. This original data set contains responses from 441,455 individuals and has 330 features. These features are either questions directly asked of participants, or calculated variables based on individual participant responses.The file we are interested in (diabetes _ binary _ health _ indicators _ BRFSS2015.csv) is a clean dataset of 253,680 survey responses to the CDC’s BRFSS2015. The target variable Diabetes_binary has 2 classes. 0 is for no diabetes, and 1 is for prediabetes or diabetes. This dataset has 21 feature variables and is not balanced. Our goal is to find how the combination of different input factors could contribute to the prediction of diabetic status.\n\n\n\nLet’s read the file into R first.\n\ndiabetesdata &lt;- read.csv (\"data\\\\diabetes_binary_health_indicators_BRFSS2015.csv\")\nstr (diabetesdata)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : num  0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num  3 1 8 6 4 8 7 4 1 3 ...\n\n\nAs shown, this data set has 22 variables, we will use only 1 Diabetes_binary as an outcome variable, and all others as predictors. However, there are many numeric variables that should actually be treated as factors. We will change those variables.\n\nlibrary (dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndiabetesdata &lt;- diabetesdata |&gt; mutate (across (-c(MentHlth, PhysHlth, BMI), as.factor))\nstr (diabetesdata)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"1\",\"2\",\"3\",\"4\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\nNow you could see the attributes of the variables have been reset. Let’s check whether are missing values in each column.\n\nmissing_counts &lt;- diabetesdata |&gt; \n  summarise(across(everything(), ~sum(is.na(.))))\nmissing_counts\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      0        0         0   0      0      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       0                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             0           0       0        0        0        0   0   0\n  Education Income\n1         0      0\n\n\nTo our relief, there is no missing data in the data set. Now we are ready to do some exploratory data analysis.\n\n\n\nWe could generate a table with incidence of diabetes and other categorical variables.\n\ntable (diabetesdata$Diabetes_binary, diabetesdata$HighBP, diabetesdata$HighChol)\n\n, ,  = 0\n\n   \n        0     1\n  0 97651 36778\n  1  4269  7391\n\n, ,  = 1\n\n   \n        0     1\n  0 38458 45447\n  1  4473 19213\n\ndiabetesdata |&gt; group_by(HighBP, HighChol, Diabetes_binary) |&gt; summarize (count = n())\n\n`summarise()` has grouped output by 'HighBP', 'HighChol'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   HighBP, HighChol [4]\n  HighBP HighChol Diabetes_binary count\n  &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;           &lt;int&gt;\n1 0      0        0               97651\n2 0      0        1                4269\n3 0      1        0               38458\n4 0      1        1                4473\n5 1      0        0               36778\n6 1      0        1                7391\n7 1      1        0               45447\n8 1      1        1               19213\n\ndiabetesdata |&gt; group_by (Smoker, HeartDiseaseorAttack, Diabetes_binary) |&gt; summarize (count = n())\n\n`summarise()` has grouped output by 'Smoker', 'HeartDiseaseorAttack'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Smoker, HeartDiseaseorAttack [4]\n  Smoker HeartDiseaseorAttack Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;                &lt;fct&gt;            &lt;int&gt;\n1 0      0                    0               118086\n2 0      0                    1                14079\n3 0      1                    0                 6142\n4 0      1                    1                 2950\n5 1      0                    0                84233\n6 1      0                    1                13389\n7 1      1                    0                 9873\n8 1      1                    1                 4928\n\n\nFrom the summary table we could easily find those factors significantly contribute to the occurance of diabetes. Is there a way to find the hazard ratio of those life-style factors to diabetes? Luckily there is a package for that.\n\nlibrary (tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndiabetesdata |&gt; group_by(HighBP, HighChol, Diabetes_binary) |&gt; summarize (count = n()) |&gt; pivot_wider(\n  names_from = Diabetes_binary,\n  values_from = count)\n\n`summarise()` has grouped output by 'HighBP', 'HighChol'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   HighBP, HighChol [4]\n  HighBP HighChol   `0`   `1`\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt; &lt;int&gt;\n1 0      0        97651  4269\n2 0      1        38458  4473\n3 1      0        36778  7391\n4 1      1        45447 19213\n\ndiabetesdata |&gt; group_by(PhysActivity,HvyAlcoholConsump, Diabetes_binary) |&gt; summarize (count = n ()) |&gt; pivot_wider (names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'PhysActivity', 'HvyAlcoholConsump'. You\ncan override using the `.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   PhysActivity, HvyAlcoholConsump [4]\n  PhysActivity HvyAlcoholConsump    `0`   `1`\n  &lt;fct&gt;        &lt;fct&gt;              &lt;int&gt; &lt;int&gt;\n1 0            0                  45847 12753\n2 0            1                   2854   306\n3 1            0                 159063 21761\n4 1            1                  10570   526\n\nsmdbtable &lt;- diabetesdata |&gt; group_by (Smoker, Diabetes_binary) |&gt; summarize (count = n()) |&gt;pivot_wider (\n  names_from = Diabetes_binary,\n  values_from = count)\n\n`summarise()` has grouped output by 'Smoker'. You can override using the\n`.groups` argument.\n\n\nWe are provided with many categorical variables with only one outcome and many others being predictors. Given diabetes are highly attributagle to life style and living condition factors, it is intuitive to make tables to calculate the hazard ratio. Luckily there is a package for that.\n\ncalculate_odds_ratio &lt;- function(data) {\n  # Compute odds ratio using contingency table\n  tab &lt;- matrix(c(data$`0`[1], data$`1`[1],\n                  data$`0`[2], data$`1`[2]),\n                nrow = 2, byrow = TRUE)\n  \n  # Use the fisher.test for OR and CI\n  test &lt;- fisher.test(tab)\n  \n  list(\n    OR = test$estimate,\n    CI = test$conf.int,\n    P_value = test$p.value\n  )\n}\n\n# Apply the function to your data\nresult &lt;- calculate_odds_ratio(smdbtable)\nprint(result)\n\n$OR\nodds ratio \n  1.419906 \n\n$CI\n[1] 1.388231 1.452338\nattr(,\"conf.level\")\n[1] 0.95\n\n$P_value\n[1] 1.406893e-204\n\n\nFor this purpose we could generate another table containing only binary factors and generate the contingency tables.\n\ndiabetesdatabin &lt;- diabetesdata |&gt; select (-BMI, -GenHlth, -MentHlth, -PhysHlth, -Age, -Education, -Income)\n\nThen we could generate the contingency table based on this.\n\ngenerate_contingency_tables &lt;- function(data, outcome) {\n  predictors &lt;- setdiff(names(data), outcome)\n  tables &lt;- lapply(predictors, function(predictor) {\n    table(data[[outcome]], data[[predictor]], dnn = c(outcome, predictor))\n  })\n  names(tables) &lt;- predictors\n  return(tables)\n}\n\ndiabetes_contingency &lt;- generate_contingency_tables (data=diabetesdatabin, outcome = \"Diabetes_binary\")\n\nHere we generated the contingency table for all binary factorial predictors. We could calculate their odds ratios.\n\nOR_HighBP &lt;- (diabetes_contingency$HighBP [2,2]/diabetes_contingency$HighBP[1,2])/(diabetes_contingency$HighBP[2,1]/diabetes_contingency$HighBP[1,1])\n\nLet’s make a function to loop through all lists in the diabetes_contigency to generate a new table called OR_table\n\ncalculate_odds_ratios &lt;- function(contingency_list) {\n  odds_ratios &lt;- lapply(contingency_list, function(table) {\n  OR_table &lt;- (table[2, 2] / table[1, 2]) / (table[2, 1] / table[1, 1])\n  return(OR_table)\n  })\n  names(odds_ratios) &lt;- names(contingency_list)\n  return(odds_ratios)\n}\ndiabetes_OR &lt;- unlist (calculate_odds_ratios (diabetes_contingency))\n\nWe could plot the odds ratio of different factors to diabetes.\n\ndiabetes_OR_df &lt;- data.frame (\n  category = names (diabetes_OR),\n  OR = as.numeric (diabetes_OR)\n)\nggplot(diabetes_OR_df, aes(x = category, y = OR)) +\n  geom_bar(stat = \"identity\", fill = ifelse(diabetes_OR_df$OR &gt; 2, \"gold\", \"skyblue\"), color = \"black\") +\n  labs(title = \"Odds ratios associated with diabetes ocurrance\", x = \"Binary Predictors\", y = \"Odds ratio\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1))\n\n\n\n\n\n\n\n\nFrom the bar plot, we could highlight the predictors with OR bigger than 2 (generally considered “big enough” in diabetic study) to be significant contributors of diabetes, including whether or not the person has experienced heart disease or attack, stroke, has high blood pressure or high cholesterol, or experienced difficulty in walking or had their cholesterol level checked in the past 5 years. Despite being the strongest predictor of diabetes (without considering the interaction among factors), Having Cholchecked still could only have about 17% of chance being diagnosed diabetes (35105 diabetes out of 209105 people), so a model combining multiple predictors and considering predictors interaction is definitely needed.\nThere are four other factors which have more than two categories. Let’s make contingency tables with those variables.\n\ndiabetesdatamult &lt;- diabetesdata |&gt; select (Diabetes_binary, GenHlth, Age, Education, Income)\ndiabetes_contingency_mult &lt;- generate_contingency_tables (data=diabetesdatamult, outcome = \"Diabetes_binary\")\ndiabetes_contingency_mult\n\n$GenHlth\n               GenHlth\nDiabetes_binary     1     2     3     4     5\n              0 44159 82703 62189 21780  7503\n              1  1140  6381 13457  9790  4578\n\n$Age\n               Age\nDiabetes_binary     1     2     3     4     5     6     7     8     9    10\n              0  5622  7458 10809 13197 15106 18077 23226 26569 27511 25636\n              1    78   140   314   626  1051  1742  3088  4263  5733  6558\n               Age\nDiabetes_binary    11    12    13\n              0 18392 12577 14154\n              1  5141  3403  3209\n\n$Education\n               Education\nDiabetes_binary     1     2     3     4     5     6\n              0   127  2860  7182 51684 59556 96925\n              1    47  1183  2296 11066 10354 10400\n\n$Income\n               Income\nDiabetes_binary     1     2     3     4     5     6     7     8\n              0  7428  8697 12426 16081 21379 31179 37954 83190\n              1  2383  3086  3568  4054  4504  5291  5265  7195\n\n\nTo find out whether there is a relationship between the incidence of diabetes and different levels of the multi-level variables, we could conduct statistical test over each contingency table.\n\ndiabeteschisq &lt;- lapply (diabetes_contingency_mult, chisq.test)\ndiabeteschisqtable &lt;- data.frame (\n  Variable = c (\"Genhlth\", \"Age\", \"Education\", \"Income\"),\n  p_value = c (diabeteschisq$GenHlth$p.value,\n               diabeteschisq$Age$p.value,\n               diabeteschisq$Educatio$p.value,\n               diabeteschisq$Income$p.value)\n  )\ndiabeteschisqtable\n\n   Variable p_value\n1   Genhlth       0\n2       Age       0\n3 Education       0\n4    Income       0\n\n\nSo all four variables have a significant effect on the incidence of diabetes as the p-values are close to 0, meaning the chances that those variables have no effect on the incidence of diabetes are close to 0. To gain a more visual impression of the variables’ impact on diabetes, we could do this:\n\ndiabetes_contingency_mult_tibble &lt;- lapply (diabetes_contingency_mult, as_tibble)\n\nGenhlth_table &lt;- diabetes_contingency_mult_tibble$GenHlth |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, GenHlth = as.factor (GenHlth))\n\nAge_table &lt;- diabetes_contingency_mult_tibble$Age |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, Age= as.factor (Age))\nAge_table$Age &lt;- factor(Age_table$Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13))\n\n\nEdu_table &lt;- diabetes_contingency_mult_tibble$Education |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, Education= as.factor (Education))\n\nIncome_table &lt;- diabetes_contingency_mult_tibble$Income |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, Income= as.factor (Income))\n\nNow we could plot them\n\nlibrary (gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nDiabetes_Genhlth_plot &lt;- \n  ggplot(Genhlth_table, aes(x = GenHlth, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"General Health\", y = \"Ratio\")\n\nDiabetes_Age_plot &lt;- \n  ggplot(Age_table, aes(x = Age, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"Age Group\", y = \"Ratio\")\n\nDiabetes_Edu_plot &lt;- \n  ggplot(Edu_table, aes(x = Education, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"pink\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"Education Level\", y = \"Ratio\")\n\nDiabetes_Income_plot &lt;- \n  ggplot(Income_table, aes(x = Income, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"gold\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"Income Level\", y = \"Ratio\")\n\ngrid.arrange(Diabetes_Genhlth_plot, Diabetes_Age_plot, Diabetes_Edu_plot, Diabetes_Income_plot, ncol = 2)\n\n\n\n\n\n\n\n\nFrom the bar plot we could tell 1) People with poorest general health is the most likely to be diagnosed with diabetes; 2) As people get older, the rate of diabetes increased but topped at age group 11, which corresponds to 70 to 74 yo. 3) People in the Education level 2 and Income level 2, but not the least-educated and poorest, are the most likely to be diagnosed diabetes. This reflects the impact of social-economic status on the incidence of diabetes.\nTo measure the overall association between different categorical variable, we could use Cramér’s V method to generate the association matrix and then plot it using pheatmeap\n\nlibrary (vcd)\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\n\ndiabetes_var &lt;- colnames(diabetesdata)\ndiabetes_association_matrix &lt;- sapply(diabetes_var, function(x) {\n  sapply(diabetes_var, function(y) {\n    cramers_v &lt;- assocstats(table(diabetesdata[[x]], diabetesdata[[y]]))$cramer\n    return(cramers_v)\n  })\n})\n\nlibrary (pheatmap)\n\nWarning: package 'pheatmap' was built under R version 4.3.2\n\npheatmap (diabetes_association_matrix, cluster_rows = FALSE, cluster_cols = FALSE,\n          color = colorRampPalette(c(\"ivory\", \"lightyellow\", \"green\", \"darkblue\"))(100),\n          breaks = seq(0, 0.4, length.out = 101),\n          main = \"Categorical Variable Association Matrix\")\n\n\n\n\n\n\n\n\nWe could see the categorical variables most highly associated with Diabetic status is “General Health”, “High BP”, “High Chol”, “High BMI”. In addition, it is also associated with “Physical health”, “Difficult to walk”, “Heart disease or attack”, “Age” and “Income”. It is of note there are several other categorical factors that have strong association that we may need to take care of when doing modeling. Nevertheless, the strongest predictor that associates with diabetic status has a assocition score of merely 0.3. Hopefully we could find the combination of predictors through modelling that could explain ~0.7 of the diabetic status.\nClick here for the Modeling Page"
  },
  {
    "objectID": "EDA.html#data-introduction",
    "href": "EDA.html#data-introduction",
    "title": "ST558-2024-Project-Final",
    "section": "",
    "text": "Diabetes is a wide-spread disease, especially in the US. Although genetic plays a role in the ethiology of diabetes, it has been estimated around 90% to 95% of diabetes cases could be attributed to life-style factors, like diet, weight management and physical activity.This data set aimed to providel links between patients’ diabetic conditions, life pattern behavior and social-economic status.\nFor this project, a csv of the data set available on Kaggle for the year 2015 was used. This original data set contains responses from 441,455 individuals and has 330 features. These features are either questions directly asked of participants, or calculated variables based on individual participant responses.The file we are interested in (diabetes _ binary _ health _ indicators _ BRFSS2015.csv) is a clean dataset of 253,680 survey responses to the CDC’s BRFSS2015. The target variable Diabetes_binary has 2 classes. 0 is for no diabetes, and 1 is for prediabetes or diabetes. This dataset has 21 feature variables and is not balanced. Our goal is to find how the combination of different input factors could contribute to the prediction of diabetic status."
  },
  {
    "objectID": "EDA.html#data-reading-and-cleaning",
    "href": "EDA.html#data-reading-and-cleaning",
    "title": "ST558-2024-Project-Final",
    "section": "",
    "text": "Let’s read the file into R first.\n\ndiabetesdata &lt;- read.csv (\"data\\\\diabetes_binary_health_indicators_BRFSS2015.csv\")\nstr (diabetesdata)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : num  0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num  3 1 8 6 4 8 7 4 1 3 ...\n\n\nAs shown, this data set has 22 variables, we will use only 1 Diabetes_binary as an outcome variable, and all others as predictors. However, there are many numeric variables that should actually be treated as factors. We will change those variables.\n\nlibrary (dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndiabetesdata &lt;- diabetesdata |&gt; mutate (across (-c(MentHlth, PhysHlth, BMI), as.factor))\nstr (diabetesdata)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"1\",\"2\",\"3\",\"4\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\nNow you could see the attributes of the variables have been reset. Let’s check whether are missing values in each column.\n\nmissing_counts &lt;- diabetesdata |&gt; \n  summarise(across(everything(), ~sum(is.na(.))))\nmissing_counts\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      0        0         0   0      0      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       0                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             0           0       0        0        0        0   0   0\n  Education Income\n1         0      0\n\n\nTo our relief, there is no missing data in the data set. Now we are ready to do some exploratory data analysis."
  },
  {
    "objectID": "EDA.html#exploratory-data-analysis",
    "href": "EDA.html#exploratory-data-analysis",
    "title": "ST558-2024-Project-Final",
    "section": "",
    "text": "We could generate a table with incidence of diabetes and other categorical variables.\n\ntable (diabetesdata$Diabetes_binary, diabetesdata$HighBP, diabetesdata$HighChol)\n\n, ,  = 0\n\n   \n        0     1\n  0 97651 36778\n  1  4269  7391\n\n, ,  = 1\n\n   \n        0     1\n  0 38458 45447\n  1  4473 19213\n\ndiabetesdata |&gt; group_by(HighBP, HighChol, Diabetes_binary) |&gt; summarize (count = n())\n\n`summarise()` has grouped output by 'HighBP', 'HighChol'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   HighBP, HighChol [4]\n  HighBP HighChol Diabetes_binary count\n  &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;           &lt;int&gt;\n1 0      0        0               97651\n2 0      0        1                4269\n3 0      1        0               38458\n4 0      1        1                4473\n5 1      0        0               36778\n6 1      0        1                7391\n7 1      1        0               45447\n8 1      1        1               19213\n\ndiabetesdata |&gt; group_by (Smoker, HeartDiseaseorAttack, Diabetes_binary) |&gt; summarize (count = n())\n\n`summarise()` has grouped output by 'Smoker', 'HeartDiseaseorAttack'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Smoker, HeartDiseaseorAttack [4]\n  Smoker HeartDiseaseorAttack Diabetes_binary  count\n  &lt;fct&gt;  &lt;fct&gt;                &lt;fct&gt;            &lt;int&gt;\n1 0      0                    0               118086\n2 0      0                    1                14079\n3 0      1                    0                 6142\n4 0      1                    1                 2950\n5 1      0                    0                84233\n6 1      0                    1                13389\n7 1      1                    0                 9873\n8 1      1                    1                 4928\n\n\nFrom the summary table we could easily find those factors significantly contribute to the occurance of diabetes. Is there a way to find the hazard ratio of those life-style factors to diabetes? Luckily there is a package for that.\n\nlibrary (tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndiabetesdata |&gt; group_by(HighBP, HighChol, Diabetes_binary) |&gt; summarize (count = n()) |&gt; pivot_wider(\n  names_from = Diabetes_binary,\n  values_from = count)\n\n`summarise()` has grouped output by 'HighBP', 'HighChol'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   HighBP, HighChol [4]\n  HighBP HighChol   `0`   `1`\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt; &lt;int&gt;\n1 0      0        97651  4269\n2 0      1        38458  4473\n3 1      0        36778  7391\n4 1      1        45447 19213\n\ndiabetesdata |&gt; group_by(PhysActivity,HvyAlcoholConsump, Diabetes_binary) |&gt; summarize (count = n ()) |&gt; pivot_wider (names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'PhysActivity', 'HvyAlcoholConsump'. You\ncan override using the `.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   PhysActivity, HvyAlcoholConsump [4]\n  PhysActivity HvyAlcoholConsump    `0`   `1`\n  &lt;fct&gt;        &lt;fct&gt;              &lt;int&gt; &lt;int&gt;\n1 0            0                  45847 12753\n2 0            1                   2854   306\n3 1            0                 159063 21761\n4 1            1                  10570   526\n\nsmdbtable &lt;- diabetesdata |&gt; group_by (Smoker, Diabetes_binary) |&gt; summarize (count = n()) |&gt;pivot_wider (\n  names_from = Diabetes_binary,\n  values_from = count)\n\n`summarise()` has grouped output by 'Smoker'. You can override using the\n`.groups` argument.\n\n\nWe are provided with many categorical variables with only one outcome and many others being predictors. Given diabetes are highly attributagle to life style and living condition factors, it is intuitive to make tables to calculate the hazard ratio. Luckily there is a package for that.\n\ncalculate_odds_ratio &lt;- function(data) {\n  # Compute odds ratio using contingency table\n  tab &lt;- matrix(c(data$`0`[1], data$`1`[1],\n                  data$`0`[2], data$`1`[2]),\n                nrow = 2, byrow = TRUE)\n  \n  # Use the fisher.test for OR and CI\n  test &lt;- fisher.test(tab)\n  \n  list(\n    OR = test$estimate,\n    CI = test$conf.int,\n    P_value = test$p.value\n  )\n}\n\n# Apply the function to your data\nresult &lt;- calculate_odds_ratio(smdbtable)\nprint(result)\n\n$OR\nodds ratio \n  1.419906 \n\n$CI\n[1] 1.388231 1.452338\nattr(,\"conf.level\")\n[1] 0.95\n\n$P_value\n[1] 1.406893e-204\n\n\nFor this purpose we could generate another table containing only binary factors and generate the contingency tables.\n\ndiabetesdatabin &lt;- diabetesdata |&gt; select (-BMI, -GenHlth, -MentHlth, -PhysHlth, -Age, -Education, -Income)\n\nThen we could generate the contingency table based on this.\n\ngenerate_contingency_tables &lt;- function(data, outcome) {\n  predictors &lt;- setdiff(names(data), outcome)\n  tables &lt;- lapply(predictors, function(predictor) {\n    table(data[[outcome]], data[[predictor]], dnn = c(outcome, predictor))\n  })\n  names(tables) &lt;- predictors\n  return(tables)\n}\n\ndiabetes_contingency &lt;- generate_contingency_tables (data=diabetesdatabin, outcome = \"Diabetes_binary\")\n\nHere we generated the contingency table for all binary factorial predictors. We could calculate their odds ratios.\n\nOR_HighBP &lt;- (diabetes_contingency$HighBP [2,2]/diabetes_contingency$HighBP[1,2])/(diabetes_contingency$HighBP[2,1]/diabetes_contingency$HighBP[1,1])\n\nLet’s make a function to loop through all lists in the diabetes_contigency to generate a new table called OR_table\n\ncalculate_odds_ratios &lt;- function(contingency_list) {\n  odds_ratios &lt;- lapply(contingency_list, function(table) {\n  OR_table &lt;- (table[2, 2] / table[1, 2]) / (table[2, 1] / table[1, 1])\n  return(OR_table)\n  })\n  names(odds_ratios) &lt;- names(contingency_list)\n  return(odds_ratios)\n}\ndiabetes_OR &lt;- unlist (calculate_odds_ratios (diabetes_contingency))\n\nWe could plot the odds ratio of different factors to diabetes.\n\ndiabetes_OR_df &lt;- data.frame (\n  category = names (diabetes_OR),\n  OR = as.numeric (diabetes_OR)\n)\nggplot(diabetes_OR_df, aes(x = category, y = OR)) +\n  geom_bar(stat = \"identity\", fill = ifelse(diabetes_OR_df$OR &gt; 2, \"gold\", \"skyblue\"), color = \"black\") +\n  labs(title = \"Odds ratios associated with diabetes ocurrance\", x = \"Binary Predictors\", y = \"Odds ratio\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1))\n\n\n\n\n\n\n\n\nFrom the bar plot, we could highlight the predictors with OR bigger than 2 (generally considered “big enough” in diabetic study) to be significant contributors of diabetes, including whether or not the person has experienced heart disease or attack, stroke, has high blood pressure or high cholesterol, or experienced difficulty in walking or had their cholesterol level checked in the past 5 years. Despite being the strongest predictor of diabetes (without considering the interaction among factors), Having Cholchecked still could only have about 17% of chance being diagnosed diabetes (35105 diabetes out of 209105 people), so a model combining multiple predictors and considering predictors interaction is definitely needed.\nThere are four other factors which have more than two categories. Let’s make contingency tables with those variables.\n\ndiabetesdatamult &lt;- diabetesdata |&gt; select (Diabetes_binary, GenHlth, Age, Education, Income)\ndiabetes_contingency_mult &lt;- generate_contingency_tables (data=diabetesdatamult, outcome = \"Diabetes_binary\")\ndiabetes_contingency_mult\n\n$GenHlth\n               GenHlth\nDiabetes_binary     1     2     3     4     5\n              0 44159 82703 62189 21780  7503\n              1  1140  6381 13457  9790  4578\n\n$Age\n               Age\nDiabetes_binary     1     2     3     4     5     6     7     8     9    10\n              0  5622  7458 10809 13197 15106 18077 23226 26569 27511 25636\n              1    78   140   314   626  1051  1742  3088  4263  5733  6558\n               Age\nDiabetes_binary    11    12    13\n              0 18392 12577 14154\n              1  5141  3403  3209\n\n$Education\n               Education\nDiabetes_binary     1     2     3     4     5     6\n              0   127  2860  7182 51684 59556 96925\n              1    47  1183  2296 11066 10354 10400\n\n$Income\n               Income\nDiabetes_binary     1     2     3     4     5     6     7     8\n              0  7428  8697 12426 16081 21379 31179 37954 83190\n              1  2383  3086  3568  4054  4504  5291  5265  7195\n\n\nTo find out whether there is a relationship between the incidence of diabetes and different levels of the multi-level variables, we could conduct statistical test over each contingency table.\n\ndiabeteschisq &lt;- lapply (diabetes_contingency_mult, chisq.test)\ndiabeteschisqtable &lt;- data.frame (\n  Variable = c (\"Genhlth\", \"Age\", \"Education\", \"Income\"),\n  p_value = c (diabeteschisq$GenHlth$p.value,\n               diabeteschisq$Age$p.value,\n               diabeteschisq$Educatio$p.value,\n               diabeteschisq$Income$p.value)\n  )\ndiabeteschisqtable\n\n   Variable p_value\n1   Genhlth       0\n2       Age       0\n3 Education       0\n4    Income       0\n\n\nSo all four variables have a significant effect on the incidence of diabetes as the p-values are close to 0, meaning the chances that those variables have no effect on the incidence of diabetes are close to 0. To gain a more visual impression of the variables’ impact on diabetes, we could do this:\n\ndiabetes_contingency_mult_tibble &lt;- lapply (diabetes_contingency_mult, as_tibble)\n\nGenhlth_table &lt;- diabetes_contingency_mult_tibble$GenHlth |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, GenHlth = as.factor (GenHlth))\n\nAge_table &lt;- diabetes_contingency_mult_tibble$Age |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, Age= as.factor (Age))\nAge_table$Age &lt;- factor(Age_table$Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13))\n\n\nEdu_table &lt;- diabetes_contingency_mult_tibble$Education |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, Education= as.factor (Education))\n\nIncome_table &lt;- diabetes_contingency_mult_tibble$Income |&gt; pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt; mutate (ratio = `1` / `0`, Income= as.factor (Income))\n\nNow we could plot them\n\nlibrary (gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nDiabetes_Genhlth_plot &lt;- \n  ggplot(Genhlth_table, aes(x = GenHlth, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"General Health\", y = \"Ratio\")\n\nDiabetes_Age_plot &lt;- \n  ggplot(Age_table, aes(x = Age, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"Age Group\", y = \"Ratio\")\n\nDiabetes_Edu_plot &lt;- \n  ggplot(Edu_table, aes(x = Education, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"pink\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"Education Level\", y = \"Ratio\")\n\nDiabetes_Income_plot &lt;- \n  ggplot(Income_table, aes(x = Income, y = ratio)) +\n  geom_bar(stat = \"identity\", fill = \"gold\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Diabetes vs non-diabetes\", x = \"Income Level\", y = \"Ratio\")\n\ngrid.arrange(Diabetes_Genhlth_plot, Diabetes_Age_plot, Diabetes_Edu_plot, Diabetes_Income_plot, ncol = 2)\n\n\n\n\n\n\n\n\nFrom the bar plot we could tell 1) People with poorest general health is the most likely to be diagnosed with diabetes; 2) As people get older, the rate of diabetes increased but topped at age group 11, which corresponds to 70 to 74 yo. 3) People in the Education level 2 and Income level 2, but not the least-educated and poorest, are the most likely to be diagnosed diabetes. This reflects the impact of social-economic status on the incidence of diabetes.\nTo measure the overall association between different categorical variable, we could use Cramér’s V method to generate the association matrix and then plot it using pheatmeap\n\nlibrary (vcd)\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\n\ndiabetes_var &lt;- colnames(diabetesdata)\ndiabetes_association_matrix &lt;- sapply(diabetes_var, function(x) {\n  sapply(diabetes_var, function(y) {\n    cramers_v &lt;- assocstats(table(diabetesdata[[x]], diabetesdata[[y]]))$cramer\n    return(cramers_v)\n  })\n})\n\nlibrary (pheatmap)\n\nWarning: package 'pheatmap' was built under R version 4.3.2\n\npheatmap (diabetes_association_matrix, cluster_rows = FALSE, cluster_cols = FALSE,\n          color = colorRampPalette(c(\"ivory\", \"lightyellow\", \"green\", \"darkblue\"))(100),\n          breaks = seq(0, 0.4, length.out = 101),\n          main = \"Categorical Variable Association Matrix\")\n\n\n\n\n\n\n\n\nWe could see the categorical variables most highly associated with Diabetic status is “General Health”, “High BP”, “High Chol”, “High BMI”. In addition, it is also associated with “Physical health”, “Difficult to walk”, “Heart disease or attack”, “Age” and “Income”. It is of note there are several other categorical factors that have strong association that we may need to take care of when doing modeling. Nevertheless, the strongest predictor that associates with diabetic status has a assocition score of merely 0.3. Hopefully we could find the combination of predictors through modelling that could explain ~0.7 of the diabetic status.\nClick here for the Modeling Page"
  }
]